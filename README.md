# GPT-2

An autocomplete language model using GPT-2 architecture. A CS 224N final project.

## Getting Started

```bash
# get setup
conda env create -f env.yml
conda activate cs224n_dfp
```

## Background

GPT-2 is a large, transformer-based language model that generates text via predicting the next word given context. We focus on building a smaller version of GPT-2 from scratch, focusing on its architecture (e.g. multi-head self-attention, position-wise feed-forward networks, byte-pair encoding for tokenization).

## Developers

[Aaron Jin](https://github.com/aaronkjin)

[Brandon Bui](https://github.com/brandonbui5)

[Eli Wandless](https://github.com/)
